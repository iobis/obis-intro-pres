{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBIS full export and gridded product\n",
    "\n",
    "OBIS also produces a full export, containing all occurrence data from OBIS packed in a single Parquet file, and a gridded product (`speciesgrids`) for very efficient queries. We will learn more about each of those in this notebook.\n",
    "\n",
    "Here we will use **R**, but all the operations can also be done through **Python**. For the `speciesgrids`, some examples with Python are available in this [repo](https://github.com/iobis/speciesgrids/tree/main/notebooks).\n",
    "\n",
    "----\n",
    "\n",
    "Hands-on outline:\n",
    "\n",
    "1. A brief introduction to Parquet \n",
    "2. Full export \n",
    "3. Gridded product \n",
    "\n",
    "Before starting, we need to install a few packages on Google Colab (if you are using Binder, just skip this cell). Also, if you are coming from the previous notebook and already installed the packages, you don't need to do it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# On your local computer you can use install.packages() for everything\n",
    "# Here we use the system interface to some, as the installation\n",
    "# with r2u is faster.\n",
    "system(\"apt-get install r-cran-sf r-cran-terra\") # Spatial packages\n",
    "system(\"apt-get install r-cran-robis\")           # OBIS API interface\n",
    "system(\"apt-get install r-cran-arrow\")           # To deal with full export\n",
    "system(\"apt-get install r-cran-DBI\")             # To deal with full export\n",
    "system(\"apt-get install r-cran-duckdb\")          # To deal with full export\n",
    "system(\"apt-get install r-cran-rnaturalearth\")   # For mapping\n",
    "system(\"apt-get install r-cran-h3jsr\")           # For indexing (H3 sytem)\n",
    "\n",
    "# Install additional from GitHub\n",
    "system(\"apt-get install r-cran-bspm\")\n",
    "bspm::enable()\n",
    "devtools::install_github(\"iobis/obistools\")\n",
    "devtools::install_github(\"ropensci/mregions2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to download some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Install AWS CLI interface, to get speciesgrids if not done\n",
    "system('curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"')\n",
    "system('unzip awscliv2.zip')\n",
    "system('sudo ./aws/install')\n",
    "system('rm awscliv2.zip')\n",
    "system('rm -r aws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Download some data that we will be using in this practical\n",
    "#### Download species grids\n",
    "fs::dir_create(\"speciesgrids\")\n",
    "system(\"aws s3 cp --recursive s3://obis-products/speciesgrids/h3_7 speciesgrids --no-sign-request\")\n",
    "\n",
    "#### Download full export sample\n",
    "system(\"aws s3 cp s3://obis-shared/training/full_export_reduced.parquet . --no-sign-request\")\n",
    "\n",
    "#### Download copernicus data sample\n",
    "system(\"aws s3 cp s3://obis-shared/training/glorys_sample.tif . --no-sign-request\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A brief introduction to Parquet\n",
    "\n",
    "This content comes from an OBIS tutorial that you can find here: [https://resources.obis.org/tutorials/arrow-obis/](https://resources.obis.org/tutorials/arrow-obis/)\n",
    "\n",
    "#### What is Parquet\n",
    "\n",
    "Parquet is a lightweight format designed for columnar storage. Its main\n",
    "difference when compared to other formats like **csv** is that Parquet\n",
    "is column-oriented (while csv is row-oriented). This means that Parquet\n",
    "is much more efficient for data accessing.To illustrate, consider the\n",
    "scenario of extracting data from a specific column in a CSV file. This\n",
    "operation entails reading through all rows across all columns. In\n",
    "contrast, Parquet enables selective access solely to the required\n",
    "column, minimizing unnecessary data retrieval. Also very important:\n",
    "Parquet files are several times lighter than csv files, improving\n",
    "storage and sharing of data. You can learn more about Parquet\n",
    "[here](https://parquet.apache.org/).\n",
    "\n",
    "<img src=\"https://resources.obis.org/tutorials/arrow-obis/image1.jpg\" width=600></img>\n",
    "\n",
    "The `Arrow` package enable us to work with Parquet files (as well some\n",
    "other interesting formats) within R. You can read the full documentation\n",
    "of the package [here](https://arrow.apache.org/docs/r/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Full export\n",
    "\n",
    "Sometimes you need to do analyses using big chunks of OBIS data. In those cases, downloading it through the API is not practical. OBIS also offer the full export of all OBIS occurrence data in `parquet` or `csv` formats.\n",
    "\n",
    "<span style=\"color: #e36400\">NOTE: the `csv` format will be deprecated in the future.</span>\n",
    "\n",
    "You can download the full export here (~15GB): https://obis.org/data/access/\n",
    "\n",
    "We will work with the `parquet` file. Loading all this data at once in your computer is usually impractical (or impossible), as it will not fit in memory. But that is not a problem! Arrow offer us many ways of loading just the structure of the file and perform filters without loading the dataset in memory. Let's explore how to do that.\n",
    "\n",
    "To save time, we downloaded a smaller version of the full export that contains just a part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(arrow)\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "\n",
    "# The reduced dataset contain only data for the class Phaeophyceae\n",
    "ds_path <- \"full_export_reduced.parquet\"\n",
    "\n",
    "ds <- open_dataset(ds_path)\n",
    "\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call `arrow::open_dataset` to open a file (or a folder with multiple files) without loading it into memory. Note that you can just see the columns and data types.\n",
    "\n",
    "As a first example, we will select only the columns `scientificName`, `family`, and `date_year`. Then, we will filter by the family **Laminariaceae**. Finally, we will group by `scientificName` and `date_year` and count the number of records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lam_records <- ds |> \n",
    "    select(scientificName, family, date_year) |> \n",
    "    filter(family == \"Laminariaceae\") |> \n",
    "    group_by(scientificName, date_year) |> \n",
    "    count() |> \n",
    "    collect()\n",
    "\n",
    "head(lam_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably noticed that at the end of our `dplyr` query we added the function **`collect()`**. This is essential! It is just at this point that `arrow` will process your request.\n",
    "\n",
    "Ok, now let's get the records for the _Laminaria_ species, and plot in a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# We can use the robis package to get the species list and the number of records\n",
    "# thus, we already filter by the top 4 species\n",
    "# You could do the same using only the export as well!\n",
    "sp_lam <- robis::checklist(\"Laminaria\")\n",
    "\n",
    "sp_lam <- sp_lam[sp_lam$taxonRank == \"Species\",]\n",
    "\n",
    "sp_lam_top4 <- sp_lam[order(sp_lam$records, decreasing = T), ][1:4,]\n",
    "\n",
    "lam_data <- ds |> \n",
    "    select(scientificName, decimalLongitude, decimalLatitude) |> \n",
    "    filter(scientificName %in% sp_lam_top4$scientificName) |> \n",
    "    collect()\n",
    "\n",
    "wrld <- rnaturalearth::ne_countries(returnclass = \"sf\")\n",
    "\n",
    "ggplot(lam_data) +\n",
    "    geom_sf(data = wrld) +\n",
    "    geom_point(aes(x = decimalLongitude, y = decimalLatitude, color = scientificName)) +\n",
    "    facet_wrap(~ scientificName) +\n",
    "    theme_light()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this works fine, for big queries it can be a bit slow, even if the file is local. Another approach that is very good is [`duckDB`](https://duckdb.org/docs/api/r.html). It not only enable to do the same queries, using SQL, but it also have a [spatial extension.](https://duckdb.org/docs/extensions/spatial/functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(DBI)\n",
    "library(duckdb)\n",
    "library(glue)\n",
    "\n",
    "con <- dbConnect(duckdb())\n",
    "dbSendQuery(con, \"install spatial; load spatial;\")\n",
    "dbSendQuery(con, \"install httpfs; load httpfs;\")\n",
    "\n",
    "query <- glue(\n",
    "    \"\n",
    "    select scientificName, count(*) as number_records\n",
    "    from read_parquet('{ds_path}')\n",
    "    where family = 'Laminariaceae'\n",
    "    group by scientificName\n",
    "    \"\n",
    ")\n",
    "\n",
    "lam_fam <- dbGetQuery(con, query)\n",
    "\n",
    "lam_fam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for spatial queries is also straightforward. We just need to read the longitude and latitude as Spatial points and then use ST_Intersects to intersect with our geometry (the WKT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# And for the case of an area is much faster because it supports spatial queries!\n",
    "wkt_area <- \"POLYGON ((-79.189453 27.293689, -79.584961 23.765237, -75.9375 22.43134, -73.959961 24.726875, -74.750977 27.176469, -79.189453 27.293689))\"\n",
    "\n",
    "query <- glue::glue(\n",
    "    \"\n",
    "  select distinct scientificName\n",
    "  from read_parquet('{ds_path}')\n",
    "  where ST_Intersects(ST_Point(decimalLongitude, decimalLatitude), ST_GeomFromText('{wkt_area}'))\n",
    "  \"\n",
    ")\n",
    "\n",
    "all_sps_area <- dbGetQuery(con, query)\n",
    "\n",
    "head(all_sps_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `speciesgrids` product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recent addition to OBIS is the `speciesgrids` product. [`speciesgrids`](https://github.com/iobis/speciesgrids) is a Python package to build WoRMS aligned combined OBIS and GBIF species distribution datasets. The resulting dataset is available in a few resolutions on AWS S3. The dataset can be downloaded locally for best performance, or queried directly from the S3 bucket. For more details about downloading and using the dataset, see the [speciesgrids README](https://github.com/iobis/speciesgrids). The dataset is served as a `parquet` file.\n",
    "\n",
    "The product aggregate all marine data from the two major databases, in a very fine grid using Uber's H3 system. H3 is an hierarchical grid system, and that enables both very fast queries and aggregations. To learn more about Uber's H3 system check [here.](https://h3geo.org/)\n",
    "\n",
    "<img src=\"https://h3geo.org/images/parent-child.png\" height=200></img>\n",
    "\n",
    "We already downloaded a local version, and we will work with that. If you instead wanted to work with the remote version you would use:\n",
    "\n",
    "```\n",
    "ds_path <- \"s3://obis-products/speciesgrids/h3_7\"\n",
    "ds <- open_dataset(ds_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ds_path <- \"speciesgrids/h3_7\"\n",
    "\n",
    "ds <- open_dataset(ds_path)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get all records for the genus **Amphiprion**, which holds the \"Nemo's\".\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a7/Peces_payaso_%28Amphiprion_bicinctus%29_en_una_an%C3%A9mona_burbuja_%28Entacmaea_quadricolor%29%2C_mar_Rojo%2C_Egipto%2C_2023-04-14%2C_DD_36.jpg\" width=300><br><span style=\"font-size: xx-small\">Photo by: Diego Delso (CC-BY-SA) from Wikimedia (https://upload.wikimedia.org/wikipedia/commons/a/a7/Peces_payaso_%28Amphiprion_bicinctus%29_en_una_an%C3%A9mona_burbuja_%28Entacmaea_quadricolor%29%2C_mar_Rojo%2C_Egipto%2C_2023-04-14%2C_DD_36.jpg)</span></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "amph_records <- ds |> \n",
    "    filter(genus == \"Amphiprion\") |> \n",
    "    collect()\n",
    "\n",
    "nrow(amph_records)\n",
    "\n",
    "head(amph_records[,c(\"species\", \"genus\", \"cell\", \"records\", \"min_year\", \"max_year\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can aggregate the data by H3 cell, to get for example the total number of species by cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "amph_total_cell <- amph_records |> \n",
    "    group_by(cell) |> \n",
    "    count()\n",
    "\n",
    "head(amph_total_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this data to calculate the [realized] thermal ranges of those species. For that, we will use an SST layer with the average SST of the period 2020-2021 from the [GLORYS12V1 product](https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/description).\n",
    "\n",
    "In this case, we don't need to use just the records from this period, but it would be interesting to avoid using records that are from years to far from the data. We will filter our records to be between 2010 and 2024. We will also keep only species with individual counts higher than 30 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "amph_filt <- amph_records |> \n",
    "    filter(min_year >= 2010 & max_year <= 2024)\n",
    "\n",
    "sp_count <- amph_filt |> \n",
    "    group_by(species) |> \n",
    "    count() |> \n",
    "    filter(n >= 30)\n",
    "\n",
    "amph_filt <- amph_filt |> \n",
    "    filter(species %in% sp_count$species)\n",
    "\n",
    "length(unique(amph_filt$species))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `speciesgrids` already contains a `geometry` column. So, let's transform this  a `sf` object using `sf::st_as_sf()` and `sf::st_as_sfc()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "coords <- sf::st_as_sf(sf::st_as_sfc(amph_filt$geometry), crs = \"EPSG:4326\")\n",
    "\n",
    "amph_filt_sf <- cbind(coords, amph_filt)\n",
    "print(amph_filt_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the SST information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(terra)\n",
    "\n",
    "sst <- rast(\"../../glorys_sample.tif\")\n",
    "\n",
    "plot(sst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 15, repr.plot.height = 10)\n",
    "extracted_sst <- terra::extract(sst, amph_filt_sf, ID = FALSE)\n",
    "\n",
    "extracted_sst$species <- amph_filt_sf$species\n",
    "head(extracted_sst)\n",
    "\n",
    "extracted_sst <- extracted_sst[!is.na(extracted_sst$mean),]\n",
    "\n",
    "# Remove a strange outlier\n",
    "extracted_sst <- extracted_sst[extracted_sst$mean > 10,]\n",
    "\n",
    "sp_limits <- extracted_sst |> \n",
    "    group_by(species) |> \n",
    "    summarise(lim = quantile(mean, .95))\n",
    "\n",
    "ggplot(extracted_sst, aes(x = species, y = mean, color = species)) +\n",
    "    geom_jitter() +\n",
    "    geom_boxplot(alpha = .5) +\n",
    "    geom_hline(yintercept = min(sp_limits$lim), color = \"#002548\", linetype = 2) +\n",
    "    geom_hline(yintercept = max(sp_limits$lim), color = \"#002548\", linetype = 2) +\n",
    "    geom_hline(yintercept = mean(sp_limits$lim), color = \"#69052c\", linetype = 2, linewidth = 1) +\n",
    "    theme_light() +\n",
    "    xlab(NULL) + ylab(\"SST\") +\n",
    "    theme(panel.grid.major.x = element_blank(),\n",
    "    panel.grid.minor.y = element_blank(),\n",
    "    axis.text.x = element_blank())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregations\n",
    "\n",
    "Because H3 system is hierarchical, we can actually aggregate it to coarser cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "amph_h3_4 <- amph_records |> \n",
    "    mutate(h3_4 = h3jsr::get_parent(cell, res = 4))\n",
    "head(amph_h3_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And calculate and plot the number of records, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "amph_h3_4_agg <- amph_h3_4 |> \n",
    "    group_by(h3_4) |> \n",
    "    summarise(total_records = sum(records))\n",
    "\n",
    "amph_h3_4_agg_sf <- h3jsr::cell_to_polygon(amph_h3_4_agg$h3_4, simple = FALSE)\n",
    "\n",
    "print(amph_h3_4_agg_sf)\n",
    "\n",
    "colnames(amph_h3_4_agg)[1] <- \"h3_address\"\n",
    "\n",
    "amph_h3_4_agg <- left_join(amph_h3_4_agg_sf, amph_h3_4_agg)\n",
    "\n",
    "wrld <- rnaturalearth::ne_countries(returnclass = \"sf\")\n",
    "\n",
    "ggplot() +\n",
    "    geom_sf(data = wrld) +\n",
    "    geom_sf(data = amph_h3_4_agg, aes(fill = total_records)) +\n",
    "    theme_light() +\n",
    "    coord_sf(xlim = c(150, 170), ylim = c(-40, -30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same operations that we did with the **full export** using `DuckDB` with this dataset. In our experience, `DuckDB` performance is much better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
